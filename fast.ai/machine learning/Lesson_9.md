# Lesson 9: Regularization, Learning Rates and NLP

## Regularization
- L1
- L2: weight decay
- dropout

### Tuning
1. We can actually modify our loss function to add in this square penalty.
2. We can modify that thing where we said weights equals weights minus gradient times learning rate to add 2aw as well.

## Learning Rates
- Learning rate annealing

## Natural Language Processing (NLP)
- Bag-of-words for IMDB
- ngram features
- Model used: Logistic Regression, Naive Bayes
- Tokenization using CountVector

## Examples of ML Application
- Coloring with Random Forest
- Parfit [here](https://medium.com/mlreview/parfit-hyper-parameter-optimization-77253e7e175e)

_Note_: Lessons 8-9 focus on writing a Logistic Regression function from scratch (bonus: Naive Bayes)
_Note_: Lesson 9 covered introduction to matrix manipulation